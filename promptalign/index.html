<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">

    <meta charset="utf-8">
    <meta property="og:title" content="PromptAlign" />
    <meta property="og:description" content="Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization" />
    <meta property="og:url" content="https://jameelhassan.github.io/promptalign/" />
<!--     <meta property="og:image" content="https://jameelhassan.github.io/promptalign/static/images/preview.png" /> -->
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="628" />
    <meta name="description"
        content="" />
    <meta name="keywords"
        content="Prompt Learning, Vision-Language models, Foundation models, Test-Time-Adaptation, Distribution Alignment" />
    <meta name="viewport" content="initial-scale=1" />
    <!-- twitter -->
<!--     <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="PromptAlign" />
    <meta name="twitter:description"
        content="" />
    <meta name="twitter:url" content="https://diffusion-classifier.github.io/" />
    <meta name="twitter:image" content="https://diffusion-classifier.github.io/static/images/preview.jpeg" />
    <meta name="twitter:site" content="@pathak2206" />
    <meta name="twitter:image" content="https://diffusion-classifier.github.io/static/images/preview.jpeg" />
    <meta name="twitter:image:src" content="https://diffusion-classifier.github.io/static/images/preview.jpeg" />
    <meta name="twitter:image_alt" content="Diffusion Classifier" /> -->

    <title>PromptAlign</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://use.typekit.net/iag3ven.css">

    <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism-coy.min.css"/> -->
    <link rel="stylesheet" href="./static/css/prism.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js">
    </script>

    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üîé</text></svg>">


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://d3js.org/d3.v3.min.js" charset="utf-8"></script>
    <script src="https://d3js.org/topojson.v1.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>

    <!-- mathjax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <p style="padding: 20px;" />
                        <h1 class="title is-1 publication-title">
                            <span id="main-title">
                                Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization
                            </span>
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <!-- TODO: FIX -->
                            <span class="author-block">
                                <a href="https://jameelhassan.github.io" target="_blank">Jameel Hassan<sup>*,</sup><sup>1</sup></a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="https://hananshafi.github.io/" target="_blank">Hanan Gani<sup>*,</sup><sup>1</sup></a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="" target="_blank">Noor Hussein<sup>1</sup></a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="https://muzairkhattak.github.io/" target="_blank">Uzair Khattak<sup>1</sup></a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <br>
                            <span class="author-block">
                                <a href="https://muzammal-naseer.netlify.app/" target="_blank">Muzammal Naseer<sup>1</sup></a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=zvaeYnUAAAAJ&hl=en" target="_blank">Fahad Khan<sup>1,</sup><sup>2</sup></a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="https://scholar.google.com.pk/citations?user=M59O9lkAAAAJ&hl=en" target="_blank">Salman Khan<sup>1,</sup><sup>3</sup></a>
                            </span>
                        </div>
                        <p style="padding: 0.25rem;" />
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Mohamed Bin Zayed University of Artificial Intelligence</span> <br>
                            &nbsp;
                            <span class="author-block"><sup>2</sup>Link√∂ping University</span>
                            &nbsp;
                            <span class="author-block"><sup>3</sup>Australian National University</span>
                            <br>
                            <span class="author-block">NeurIPS 2023</span>
                            <!-- <br style="line-height: 2px" /> -->
                            <!-- <span class="author-block" style="font-size: 0.7em; font-style: italic;"><sup>*</sup>Equal
                                contribution</span> -->

                        </div>

                        <p style="padding: 20px;" />

                        <div class="buttons is-centered">
                            <button class="external-link button is-medium is-ghost publication-links is-rounded">
                                <a href="" target="_blank"
                                    style="text-decoration:none;">
                                    <span class="icon is-small">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a>
                            </button>
                            <button class="external-link button is-medium is-ghost publication-links is-rounded">
                                <a href="" target="_blank">
                                    <span class="icon is-small">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>pdf</span>
                                </a>
                            </button>
                            <button class="external-link button is-medium is-ghost publication-links is-rounded">
                                <a href="https://github.com/jameelhassan/PromptAlign" target="_blank">
                                    <span class="icon is-small">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>code</span>
                                </a>
                            </button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- hack to pull the below up vertically -->
    <span style="display:block; margin-top:-1.75em;"/>

        <!-- Method Overview -->
    <section class="section" id="method-overview">
        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column" style="border-radius: 10px; background-color: rgb(245,245,245)">
                    <h2 class="title is-3">
                        <span class="method-name">"Prompt Align"</span>
                    </h2>
                    <p style="padding: 10px;" />
                    <div id="method-overview-wrapper">
                        <img src="./static/images/conceptdiagram.png" alt="PromptAlign Concept."
                            class="method-overview-full-img  method-overview" draggable="false" />
                    </div>
                            <p style="padding: 10px;" />
                        <div class="method-overview-text has-text-justified">
                            <p>
                                PromptAlign matches the distribution statistics <strong> Œº<sub>ùëô</sub>(œÑ ; ùê©) </strong>, 
                                <strong> œÉ¬≤<sub>ùëô</sub>(œÑ ; ùê©) </strong>, obtained from multiple augmented views of a single test sample, 
                                with the source data distribution statistics <strong>ŒºÃÇ<sub>ùëô</sub></strong>, <strong>œÉÃÇ¬≤<sub>ùëô</sub></strong>. 
                                This effectively brings the test sample closer to the distribution of the source data, where the domain shift 
                                is denoted by <span style="color: purple; font-size: 1em;">‚ñ≤<sub>1</sub></span> ‚Üí 
                                <span style="color: purple; font-size: 1em;">‚ñ≤<sub>2</sub></span>. 
                                <strong>œÑ</strong> denotes the distribution of the test sample, <strong>ùê©</strong> represents the prompts that are 
                                updated, and <strong>ùëô</strong> refers to the vision-backbone layers. <strong>(b)</strong> Owing to the distribution 
                                matching via prompts, PromptAlign surpasses the existing state-of-the-art prompt learning approaches 
                                on 8 out of 10 datasets in cross-dataset generalization benchmarks. 
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!--/ Method Overview -->

    <!-- <p style="padding: 10px;" /> -->

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            The promising zero-shot generalization of vision-language models such as CLIP
                            has led to their adoption using prompt learning for numerous downstream tasks.
                            Previous works have shown test-time prompt tuning using entropy minimization
                            to adapt text prompts for unseen domains. While effective, this overlooks the key
                            cause for performance degradation to unseen domains ‚Äì distribution shift. In this
                            work, we explicitly handle this problem by aligning the out-of-distribution (OOD)
                            test sample statistics to those of the source data using prompt tuning. We use a
                            single test sample to adapt multi-modal prompts at test time by minimizing the
                            feature distribution shift to bridge the gap in the test domain. Evaluating against the
                            domain generalization benchmark, our method improves zero-shot top-1 accuracy
                            beyond existing prompt-learning techniques, with a 3.08% improvement over the
                            baseline MaPLe. In cross-dataset generalization with unseen categories across 10
                            datasets, our method improves by 1.82% compared to the existing state-of-the-art.
                            Our source code and models will be publicly released.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->


            <!-- Paper video. -->
            <!-- <p style="padding: 20px;" /> -->
            <!-- <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Video</h2>
                    <div class="publication-video">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/1hYtGZ0CUSA"
                            title="YouTube video player" frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                            allowfullscreen></iframe>
                    </div>
                </div>
            </div> -->
            <!--/ Paper video. -->
        </div>
    </section>

    <p style="padding: 20px;" />

    <!-- Derivation. -->
    <section class="section">
        <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <h2 class="title is-3">PromptAlign Design</h2>
                    <div class="column is-three-fourths">
                        <figure>
                            <img src="./static/images/arch.png" alt="PromptAlign design" id="design-image"
                                draggable="false" />
                            <figcaption>
                                Architecture and design of PromptAlign.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            <p style="padding: 20px;" />
                
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <div class="content has-text-justified">
                        <p class="equation-text">
                            PromptAlign explicitly aligns the token disctribution statistics for each test sample with that of the source data
                            statistics. The source data statistics are computed using ImageNet as a proxy dataset, given by: 
                        </p>
                        <p class="equation">
                            <script type="math/tex">
                                 \bm{\hat{\mu}}_{l} =  \bm{\mu}_{l}(\mathcal{D}, {\theta}_v)  \quad  \text{and} \quad  \bm{\hat{\sigma}^2}_{l} =  \bm{\sigma}^2_{l}(\mathcal{D}, {\theta}_v) \quad
                                \label{eq:source-stats}
                            </script>
                        </p>
                        
                        <p class="equation-text">
                            At test time, multiple augmented views of the sample are passed through the CLIP model and the token 
                            distribution statistics -- mean and variance -- are computed as:
                        </p>
                        
                        <p class="equation">
                            <script type="math/tex">
                                \bm{\mu}_{l}(\mathcal{T} ; \bm{p}) = \frac{1}{N_k} \sum_{\mathrm{x} \in \mathcal{H}(X)}  \bm{\Tilde{X}^p}_{l, \mathrm{x}} \quad ,
                                \bm{\sigma^2}_{l}(\mathcal{T} ; \bm{p}) = \frac{1}{N_k} \sum_{\mathrm{x} \in \mathcal{H}(X)} \bigg(\bm{\Tilde{X}^p}_{l, \mathrm{x}} - \bm{\mu}_{l}(\mathcal{T} ; \bm{p})\bigg)^2 ,
                                \label{eq:test-stats}
                            </script>
                        </p>
                        <p class="equation-text">
                            The distribution alignment loss is computed between the offline computed source data statistics and the test sample 
                            statistics across the transformer layers for all tokens as in \ref{eq:align-loss}. The resulting alignment loss from the 
                            distribution shift is combined with the entropy loss to update the multi-modal prompts. For each sample, a single 
                            update of the prompts are done, and it is reset to the original prompts for the next sample.
                        </p>
                        <p class="equation">
                            <script type="math/tex">
                                \label{eq:align-loss}
                                \mathcal{L}_{\text{align}} = \frac{1}{L}\sum_{l=1}^{L} \bigg( \|  \bm{\mu}_{l}(\mathcal{T} ; \bm{p}) - \bm{\hat{\mu}}_{l} \|_1  +  \| \bm{\sigma^2}_{l}(\mathcal{T} ; \bm{p}) -   \bm{\hat{\sigma}^2}_{l}\|_1 \bigg).
                            </script>
                        </p>
                        
                    </div>
                </div>
            </div>
            
        
        </div>
    </section>

    <p style="padding: 20px;" />
    <section class="section">
        <!-- Zero Shot. -->
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Zero-shot Classification</h2>
                    <div class="content has-text-justified">
                        <p>
                            We build <span class="method-name">Diffusion Classifier</span> on top of
                            <a target="_blank" href="https://github.com/Stability-AI/stablediffusion">Stable
                                Diffusion</a>, a
                            text-to-image latent diffusion model trained on a filtered subset of <a target="_blank"
                                href="https://laion.ai/blog/laion-5b/">LAION-5B</a>.
                            Our zero-shot classification method is
                            competitive with CLIP and significantly outperforms the zero-shot diffusion model baseline
                            that trains a
                            classifier on synthetic SD data. It also generally outperforms the baseline trained on
                            Stable Diffusion
                            features, especially on complex datasets like ImageNet. This is especially impressive since
                            the "SD Features"
                            baseline uses the entire training set to train a classifier.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <p style="padding: 20px;" />
        <div class="is-centered has-text-centered">
            <div class="table-container is-max-desktop">
                <table style="width:100%">
                    <caption>
                        Zero-shot classification performance on a suite of tasks.
                    </caption>
                    <tr>
                        <th></th>
                        <th>Zero-shot?</th>
                        <th>Food</th>
                        <th>CIFAR10</th>
                        <th>FGVC</th>
                        <th>Pets</th>
                        <th>Flowers</th>
                        <th>STL10</th>
                        <th>ImageNet</th>
                        <th>ObjectNet</th>
                    </tr>
                    <tr>
                        <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                    </tr>
                    <tr>
                        <td>Synthetic SD Data</td>
                        <!-- <td>&#10003;</td> -->
                        <td style="color:lime">&#10003;</td>
                        <td>12.6</td>
                        <td>35.3</td>
                        <!-- add spaces to make it render inline -->
                        <td>&nbsp;&nbsp;&nbsp;9.4</td>
                        <td>31.3</td>
                        <td>22.1</td>
                        <td>38.0</td>
                        <td>18.9</td>
                        <td>&nbsp;&nbsp;&nbsp;5.2</td>
                    </tr>
                    <tr>
                        <td>SD Features</td>
                        <td style="color:red">&#10007;</td>
                        <td style="color:lightgray">73.0</td>
                        <td style="color:lightgray">84.0</td>
                        <td style="color:lightgray"><b>35.2</b></td>
                        <td style="color:lightgray">75.9</td>
                        <td style="color:lightgray"><b>70.0</b></td>
                        <td style="color:lightgray">87.2</td>
                        <td style="color:lightgray">56.6</td>
                        <td style="color:lightgray">10.2</td>
                    </tr>
                    <tr>
                        <td><span class="method-name">Diffusion Classifier</span></td>
                        <td style="color:lime">&#10003;</td>
                        <td><b>77.7</b></td>
                        <td><b>88.5</b></td>
                        <td>26.4</td>
                        <td><b>87.3</b></td>
                        <td>66.3</td>
                        <td><b>95.4</b></td>
                        <td><b>61.4</b></td>
                        <td><b>43.4</b></td>
                    </tr>
                    <tr>
                        <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                    </tr>
                    <tr>
                        <td>CLIP ResNet50</td>
                        <td style="color:lime">&#10003;</td>
                        <td>81.1</td>
                        <td>75.6</td>
                        <td>19.3</td>
                        <td>85.4</td>
                        <td>65.9</td>
                        <td>94.3</td>
                        <td>58.2</td>
                        <td>40.0</td>
                    </tr>
                    <tr>
                        <td>OpenCLIP ViT-H/14</td>
                        <td style="color:lime">&#10003;</td>
                        <td>92.7</td>
                        <td>97.3</td>
                        <td>42.3</td>
                        <td>94.6</td>
                        <td>79.9</td>
                        <td>98.3</td>
                        <td>76.8</td>
                        <td>69.2</td>
                    </tr>
                </table>
            </div>
        </div>
        <!--/ Zero Shot. -->
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Compositional Reasoning</h2>
                    <div class="content has-text-justified">
                        <p>
                            We compare our zero-shot <span class="method-name">Diffusion Classifier</span> method to
                            CLIP and OpenCLIP on
                            <a href="https://arxiv.org/abs/2204.03162" target="_blank">Winoground</a>,
                            a popular benchmark for evaluating the visio-linguistic compositional reasoning abilities of
                            vision-language models. This benchmark tests whether models can match captions to the
                            correct images when certain entities are swapped in the captions.
                        </p>
                    </div>
                </div>
            </div>
            <p style="padding: 10px;" />
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-three-fourths">
                        <figure>
                            <img src="./static/images/winoground.jpeg" alt="Winoground examples" id="winoground-image"
                                draggable="false" />
                            <figcaption>
                                Results on selected Winoground image-caption pairs.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
            <p style="padding: 20px;" />
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <div class="content has-text-justified">
                        <p>
                            <span class="method-name">Diffusion Classifier</span> significantly outperforms both contrastive baselines. 
                            Since Stable Diffusion uses the same text
                            encoder as OpenCLIP ViT-H/14, this improvement must come from better cross-modal binding of
                            concepts to images. Overall, we find it surprising that Stable Diffusion, trained with only
                            sample generation in mind, can be repurposed into such a good classifier and reasoner.
                        </p>
                    </div>
                </div>
            </div>
            <div class="table-container is-max-desktop is-centered">
                <table style="width:70%">
                    <caption>
                        Zero-shot reasoning results on Winoground
                    </caption>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Object</th>
                            <th>Relation</th>
                            <th>Both</th>
                            <th>Average</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                        </tr>
                        <tr>
                            <td>Random Chance</td>
                            <td>25.0</td>
                            <td>25.0</td>
                            <td>25.0</td>
                            <td>25.0</td>
                        </tr>
                        <tr>
                            <td>CLIP ViT-L/14</td>
                            <td>27.0</td>
                            <td>25.8</td>
                            <td>57.7</td>
                            <td>28.2</td>
                        </tr>
                        <tr>
                            <td>OpenCLIP ViT-H/14</td>
                            <td>39.0</td>
                            <td>26.6</td>
                            <td>57.7</td>
                            <td>33.0</td>
                        </tr>
                        <tr>
                            <td><span class="method-name">Diffusion Classifier</span></td>
                            <td><strong>46.1</strong></td>
                            <td><strong>29.2</strong></td>
                            <td><strong>80.8</strong></td>
                            <td><strong>38.5</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Strong Standard Classification Ability</h2>
                    <div class="content has-text-justified">
                        <p>
                            We use <span class="method-name">Diffusion Classifier</span> to obtain a standard 1000-way
                            classifier on ImageNet from a pretrained <a target="_blank"
                                href="https://arxiv.org/abs/2303.16203">Diffusion Transformer</a> (DiT) model. DiT is a
                            class-conditional diffusion model trained solely on ImageNet-1k, with only random horizontal
                            flips and no regularization. We compare Diffusion Classifier in this setting to strong
                            discriminative classifiers like ResNet-101 and ViT-B/16 in the table below.
                            We highlight cells in green where Diffusion Classifier outperforms.
                        </p>
                    </div>
                </div>
            </div>
            <div class="table-container is-max-desktop is-centered">
                <table>
                    <caption>
                        Diffusion Classifier performs well ID and OOD.
                    </caption>
                    <thead>
                        <tr>
                            <th rowspan="2">Method</th>
                            <th colspan="1"  style="text-align: right !important;">ID</th>
                            <th colspan="3" style="text-align: center !important;">OOD</th>
                        </tr>
                        <tr>
                            <th>IN</th>
                            <th>IN-v2</th>
                            <th>IN-A</th>
                            <th>ObjectNet</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td colspan="11" style="border-bottom: 1px solid #ddd;"></td>
                        </tr>
                        <tr>
                            <td>ResNet-18</td>
                            <td class="table-highlight">70.3</td>
                            <td class="table-highlight">57.3</td>
                            <td class="table-highlight">1.1</td>
                            <td class="table-highlight">27.2</td>
                        </tr>
                        <tr>
                            <td>ResNet-34</td>
                            <td class="table-highlight">73.8</td>
                            <td class="table-highlight">61.0</td>
                            <td class="table-highlight">1.9</td>
                            <td class="table-highlight">31.6</td>
                        </tr>
                        <tr>
                            <td>ResNet-50</td>
                            <td class="table-highlight">76.7</td>
                            <td class="table-highlight">63.2</td>
                            <td class="table-highlight">0.0</td>
                            <td>36.4</td>
                        </tr>
                        <tr>
                            <td>ResNet-101</td>
                            <td class="table-highlight">77.7</td>
                            <td class="table-highlight">65.5</td>
                            <td class="table-highlight">4.7</td>
                            <td>39.1</td>
                        </tr>
                        <tr>
                            <td>ViT-L/32</td>
                            <td class="table-highlight">77.9</td>
                            <td class="table-highlight">64.4</td>
                            <td class="table-highlight">11.9</td>
                            <td class="table-highlight">32.1</td>
                        </tr>
                        <tr>
                            <td>ViT-L/16</td>
                            <td>80.4</td>
                            <td>67.5</td>
                            <td class="table-highlight">16.7</td>
                            <td>36.8</td>
                        </tr>
                        <tr>
                            <td>ViT-B/16</td>
                            <td>81.2</td>
                            <td>69.6</td>
                            <td class="table-highlight">20.8</td>
                            <td>39.9</td>
                        </tr>
                        <tr>
                            <td><span class="method-name">Diffusion Classifier (256x256)</span></td>
                            <td>77.5</td>
                            <td>64.6</td>
                            <td>20.0</td>
                            <td>32.1</td>
                        </tr>
                        <tr>
                            <td><span class="method-name">Diffusion Classifier (512x512)</span></td>
                            <td>79.1</td>
                            <td>66.7</td>
                            <td>30.2</td>
                            <td>33.9</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <div class="content has-text-justified">
                        <p>
                            Diffusion Classifier achieves 79.1% top-1 accuracy on ImageNet, which is stronger than ResNet-101 and ViT-L/32.
                            <strong>To the best of our knowledge, our approach is the first generative modeling approach
                            to achieve ImageNet accuracy comparable with to highly competitive discriminative classifiers.</strong>
                            This is impressive since the discriminative models are trained with highly tuned learning rate
                            schedules, augmentation strategies, and regularization. 
                        </p>
                    </div>
                </div>
            </div>
            <p style="padding: 10px;" />
            <div class="container is-three-quarters">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-three-quarters">
                        <figure>
                            <img src="./static/images/imagenetA_robustness.svg" alt="Effective robustness on ImageNet-A" id="robustness-image"
                                draggable="false" />
                            <figcaption>
                                Diffusion Classifier exhibits "effective robustness," where it achieves much better OOD accuracy than expected based on its ID accuracy.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </div>
        </div>
    </section>

    <section class="section" id="paper">
        <div class="container is-mobile">
            <div class="columns is-centered has-text-centered">
                <div class="container content">
                    <h2 class="title is-3">BibTeX</h2>
                    <div id="bibtex" class="column has-text-justified is-centered">
                        <!-- https://github.com/SaswatPadhi/prismjs-bibtex -->
                        <pre><code class="language-bibtex">@misc{li2023diffusion,
    title={Your Diffusion Model is Secretly a Zero-Shot Classifier}, 
    author={Alexander C. Li and Mihir Prabhudesai and Shivam Duggal and Ellis Brown and Deepak Pathak},
    year={2023},
    eprint={2303.16203},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}</code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <!-- TODO: UPDATE -->
                <a class="icon-link" href="https://arxiv.org/abs/2303.16203" target="_blank">
                    <i class="ai ai-arxiv"></i>
                </a>
                &nbsp;
                <!-- TODO: UPDATE -->
                <a class="icon-link" href="./static/docs/DiffusionClassifier.pdf" target="_blank">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <!-- &nbsp;
                <a class="icon-link" href="https://youtu.be/1hYtGZ0CUSA" target="_blank">
                    <i class="fab fa-youtube"></i>
                </a> -->
                <!-- &nbsp;
                <a class="icon-link" href="./static/docs/InternetExplorer.pptx" target="_blank">
                    <i class="fas fa-file-powerpoint"></i>
                </a> -->
                &nbsp;
                <a class="icon-link" href="https://github.com/diffusion-classifier/diffusion-classifier"
                    target="_blank">
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        Page source code was adapted from
                        <a href="https://nerfies.github.io" target="_blank">here</a>
                        and
                        <a href="https://internet-explorer-ssl.github.io"
                            target="_blank">here</a>,
                        and can be found in <a
                            href="https://github.com/diffusion-classifier/diffusion-classifier.github.io"
                            target="_blank">this repository</a>.
                    </p>
                </div>
            </div>
    </footer>

    <script src="./static/js/index.js"></script>
    <script src="./static/js/prism.js"></script>
<!--     <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.js"
        integrity="sha256-+dK6uqUp/DnP6ef97s8XcoynBnGe5vM5gvBECH0EB3U=" crossorigin="anonymous">
        </script> -->
</body>

</html>
